# -*- coding: utf-8 -*-
"""cardiomegaly_CNN_grayscale_with regularization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zselLIHR3jif3719pF2obagwP3BcN-fk
"""

import tifffile as tiff
import matplotlib.pyplot as plt
import numpy as np
from google.colab import drive
drive.mount('/content/drive/')
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import regularizers
from keras.layers import BatchNormalization

from keras.preprocessing.image import ImageDataGenerator

img_height, img_width = 256, 256

train_data_dir = '/content/drive/MyDrive/Colab Notebooks/data/train/'
validation_data_dir = '/content/drive/MyDrive/Colab Notebooks/data/val'
test_data_dir = '/content/drive/MyDrive/Colab Notebooks/data/test'

# normalize the pixel values from [0, 255] to [0, 1] interval
datagen = ImageDataGenerator(rescale=1./255)

# automagically retrieve images and their classes for train and validation sets
batch_size = 32 

train_generator = datagen.flow_from_directory(
        train_data_dir,
        target_size=(img_width, img_height),
        color_mode = 'grayscale',
        batch_size=batch_size,
        class_mode='categorical')

validation_generator = datagen.flow_from_directory(
        validation_data_dir,
        target_size=(img_width, img_height),
        color_mode = 'grayscale',
        batch_size=batch_size,
        class_mode='categorical')

from keras.models import Sequential
from keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D
from tensorflow.keras.optimizers import SGD
from keras.callbacks import ModelCheckpoint


img_width, img_height = 256,256
input_dim = 256*256
output_dim = 2

def CNN():
    model = Sequential()
    model.add(Conv2D(32, (2,2), input_shape = (img_width, img_height,1), kernel_initializer='normal',kernel_regularizer = regularizers.l1(1e-6),activation = 'relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(64, (2,2), activation = 'relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(64, (2,2), activation = 'relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(500, activation = 'relu'))
    model.add(BatchNormalization())
    model.add(Dense(500, activation = 'relu'))
    model.add(BatchNormalization())
    model.add(Dense(500, activation = 'relu'))
    model.add(BatchNormalization())
    model.add(Dense(500, activation = 'relu'))
    model.add(BatchNormalization())
    model.add(Dense(2, activation = 'softmax')) 
    return model

model = CNN()

model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy']) # compile model

import os

# print model information
model.summary()
nb_epoch = 10
outputFolder = 'weights/'
if not os.path.exists(outputFolder):
    os.makedirs(outputFolder)

filepath=outputFolder+"/weights-{epoch:02d}-{val_accuracy:.2f}.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, \
                             save_best_only=False, save_weights_only=True, \
                             mode='auto', period=1,save_freq= "epoch")
callbacks_list = [checkpoint]




model_info = model.fit(
        train_generator,
        epochs=nb_epoch,
        batch_size=100,
        validation_data=validation_generator,
        validation_batch_size=100,
        callbacks=callbacks_list)

import matplotlib.pyplot as plt
import numpy as np

def plot_model_history(model_history):
    fig, axs = plt.subplots(1,2,figsize=(15,5))
    # model history for accuracy
    axs[0].plot(range(1,len(model_history.history['accuracy'])+1),model_history.history['accuracy'])
    axs[0].plot(range(1,len(model_history.history['val_accuracy'])+1),model_history.history['val_accuracy'])
    axs[0].set_title('Model Accuracy')
    axs[0].set_ylabel('Accuracy')
    axs[0].set_xlabel('Epoch')
    axs[0].set_xticks(np.arange(1,len(model_history.history['accuracy'])+1),len(model_history.history['accuracy'])/10)
    axs[0].legend(['training acc', 'val acc'], loc='best')
    axs[0].grid(True)
    # model history for loss
    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])
    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])
    axs[1].set_title('Model Loss')
    axs[1].set_ylabel('Loss')
    axs[1].set_xlabel('Epoch')
    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)
    axs[1].legend(['training loss', 'val loss'], loc='best')
    axs[1].grid(True)
    plt.show()
plot_model_history(model_info)



model.save("/content/drive/MyDrive/Colab Notebooks/model.h5", include_optimizer=True)
print("Saved model to disk")

_, training_accuracy = model.evaluate_generator(train_generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)
print('training accuracy: %.2f' % (training_accuracy*100))

_, validation_accuracy = model.evaluate(validation_generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)
print('validation accuracy: %.2f' % (validation_accuracy*100))

test_generator = datagen.flow_from_directory(
        test_data_dir,
        target_size=(img_width, img_height),
        color_mode = 'grayscale',
        batch_size=batch_size,
        class_mode='categorical')
# test_batches = datagen.flow_from_directory(directory=test_data_dir, target_size=(256,256), classes=['Grade0', 'Grade1','Grade2','Grade3','Grade4'], batch_size=10, shuffle=False)

_, test_accuracy = model.evaluate_generator(test_generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)

print('test accuracy: %.2f' % (test_accuracy*100))

