# -*- coding: utf-8 -*-
"""cardiomegaly_CNN_grayscale_pervious.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ojhsfXfm8Qa_K1sTNfoDXs9vuaNQYo53
"""

import tifffile as tiff
import matplotlib.pyplot as plt
import numpy as np
from google.colab import drive
drive.mount('/content/drive/')
from keras.preprocessing.image import ImageDataGenerator

from keras.preprocessing.image import ImageDataGenerator

img_height, img_width = 256, 256

train_data_dir = '/content/drive/MyDrive/Colab Notebooks/dataset_k1/train/'
validation_data_dir = '/content/drive/MyDrive/Colab Notebooks/dataset_k1/val/'
test_data_dir = '/content/drive/MyDrive/Colab Notebooks/dataset_k1/test/'

# normalize the pixel values from [0, 255] to [0, 1] interval
datagen = ImageDataGenerator(rescale=1./255)

# automagically retrieve images and their classes for train and validation sets
batch_size = 32 

train_generator = datagen.flow_from_directory(
        train_data_dir,
        target_size=(img_width, img_height),
        color_mode = 'grayscale',
        batch_size=batch_size,
        class_mode='categorical')

validation_generator = datagen.flow_from_directory(
        validation_data_dir,
        target_size=(img_width, img_height),
        color_mode = 'grayscale',
        batch_size=batch_size,
        class_mode='categorical')

from keras.models import Sequential
from keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D
from tensorflow.keras.optimizers import SGD
from keras.callbacks import ModelCheckpoint


img_width, img_height = 256,256
input_dim = 256*256
output_dim = 2

def CNN():
    model = Sequential()
    model.add(Conv2D(32, (2,2), input_shape = (img_width, img_height,1), activation = 'relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(64, (2,2), activation = 'relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(64, (2,2), activation = 'relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(500, activation = 'relu'))
    model.add(Dense(500, activation = 'relu'))
    model.add(Dense(500, activation = 'relu'))
    model.add(Dense(500, activation = 'relu'))
    model.add(Dense(2, activation = 'sigmoid')) 
    return model

model = CNN()

model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) # compile model

import os

# print model information
model.summary()
nb_epoch = 10
outputFolder = 'weights/'
if not os.path.exists(outputFolder):
    os.makedirs(outputFolder)

filepath=outputFolder+"/weights-{epoch:02d}-{val_accuracy:.2f}.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, \
                             save_best_only=False, save_weights_only=True, \
                             mode='auto', period=1,save_freq= "epoch")
callbacks_list = [checkpoint]




model_info = model.fit(
        train_generator,
        epochs=nb_epoch,
        batch_size=100,
        validation_data=validation_generator,
        validation_batch_size=100,
        callbacks=callbacks_list)

import matplotlib.pyplot as plt
import numpy as np

def plot_model_history(model_history):
    fig, axs = plt.subplots(1,2,figsize=(15,5))
    # model history for accuracy
    axs[0].plot(range(1,len(model_history.history['accuracy'])+1),model_history.history['accuracy'])
    axs[0].plot(range(1,len(model_history.history['val_accuracy'])+1),model_history.history['val_accuracy'])
    axs[0].set_title('Model Accuracy')
    axs[0].set_ylabel('Accuracy')
    axs[0].set_xlabel('Epoch')
    axs[0].set_xticks(np.arange(1,len(model_history.history['accuracy'])+1),len(model_history.history['accuracy'])/10)
    axs[0].legend(['training acc', 'val acc'], loc='best')
    axs[0].grid(True)
    # model history for loss
    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])
    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])
    axs[1].set_title('Model Loss')
    axs[1].set_ylabel('Loss')
    axs[1].set_xlabel('Epoch')
    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)
    axs[1].legend(['training loss', 'val loss'], loc='best')
    axs[1].grid(True)
    plt.show()
plot_model_history(model_info)

model.save("/content/drive/MyDrive/Colab Notebooks/project/model/model.h5", include_optimizer=True)
print("Saved model to disk")

_, training_accuracy = model.evaluate_generator(train_generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)
print('training accuracy: %.2f' % (training_accuracy*100))

_, validation_accuracy = model.evaluate(validation_generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)
print('validation accuracy: %.2f' % (validation_accuracy*100))

test_generator = datagen.flow_from_directory(
        test_data_dir,
        target_size=(img_width, img_height),
        color_mode = 'grayscale',
        batch_size=batch_size,
        class_mode='categorical')
# test_batches = datagen.flow_from_directory(directory=test_data_dir, target_size=(256,256), classes=['Grade0', 'Grade1','Grade2','Grade3','Grade4'], batch_size=10, shuffle=False)

_, test_accuracy = model.evaluate_generator(test_generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)

print('test accuracy: %.2f' % (test_accuracy*100))

import cv2
r2=cv2.imread('/content/drive/MyDrive/Colab Notebooks/project/dataset/train/cardio_segmented_bounded/00000448_000.png',0)

count_r=0
count_l=0
im=r2
r,c=im.shape
x=round(3*(r2.shape[0]/4 ))
y=round(r2.shape[1]/2)
flag=0
for i in range(y,c):
  if r2[x,i]==255:
      flag=1
      k=i
      break
  else:
    count_r=count_r+1
    #r2[x,i]=255
    

for j in range(y-1,1,-1):
  if r2[x,j]==255:
      flag=1
      t=j
      break
  else:
    #r2[x,i]=255
    count_l= count_l +1
plt.imshow(r2,cmap='gray')
for s in range(t,k):
  im[x,s]=255
plt.imshow(im,cmap='gray')
print(count_l)
print(count_r)
total_count  = count_r + count_l

print(c)
CTR=0
CTR=(total_count/c*100)
CTR

root_dir='/content/drive/MyDrive/Colab Notebooks/project/dataset/val/normal_segmented_bounded'
import os
import cv2
list_per=[]
list_img=[]
images=os.listdir(root_dir)
for i, image_name in enumerate(images):
  x=0
  y=0
  t=0
  k=0
  if image_name.endswith(".png"):
    r2 = cv2.imread(root_dir+"/"+image_name,0)
    list_img.append(image_name)
    count_r=0
    count_l=0
    im=r2
    r,c=im.shape
    x=round(3*(r2.shape[0]/4 ))
    y=round(r2.shape[1]/2)
    flag=0
    for i in range(y,c):
      if r2[x,i]==255:
        flag=1
        k=i
        break
      else:
        count_r=count_r+1
        #r2[x,i]=255
    for j in range(y-1,1,-1):
      if r2[x,j]==255:
        flag=1
        t=j
        break
      else:
        #r2[x,i]=255
        count_l= count_l +1
    for s in range(t,k):
      im[x,s]=255
    total_count  = count_r + count_l
    CTR=0
    CTR=(total_count/c*100)
    list_per.append(CTR)

list_img

list_per

